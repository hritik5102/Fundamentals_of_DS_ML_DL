{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN-From-Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMDpk5uDWum7",
        "colab_type": "text"
      },
      "source": [
        "### **Neural Network from scratch**\n",
        "\n",
        "We are building a basic deep neural network with 4 layers in total: 1 input layer, 2 hidden layers and 1 output layer. All layers will be fully connected.\n",
        "\n",
        "We are making this neural network, because we are trying to classify digits from 0 to 9, using a dataset called MNIST, that consists of 70000 images that are 28 by 28 pixels. The dataset contains one label for each image, specifying the digit we are seeing in each image. We say that there are 10 classes, since we have 10 labels.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://mlfromscratch.com/content/images/2020/03/mnist-1.png\" width=\"60%\"/>\n",
        "\n",
        "For training the neural network, we will use stochastic gradient descent; which means we put one image through the neural network at a time.\n",
        "\n",
        "Let's try to define the layers in an exact way. To be able to classify digits, we must end up with the probabilities of an image belonging to a certain class, after running the neural network, because then we can quantify how well our neural network performed.\n",
        "\n",
        "Input layer: In this layer, we input our dataset, consisting of 28x28 images. We flatten these images into one array with $28 \\times 28 = 784$ elements. This means our input layer will have 784 nodes.\n",
        "Hidden layer 1: In this layer, we have decided to reduce the number of nodes from 784 in the input layer to 128 nodes. This brings a challenge when we are going forward in the neural network (explained later).\n",
        "Hidden layer 2: In this layer, we have decided to go with 64 nodes, from the 128 nodes in the first hidden layer. This is no new challenge, since we already reduced the number in the first layer.\n",
        "Output layer: In this layer, we are reducing the 64 nodes to a total of 10 nodes, so that we can evaluate the nodes against the label. This label is received in the form of an array with 10 elements, where one of the elements is 1, while the rest is 0. <br/>\n",
        "\n",
        "You might realize that the number of nodes in each layer decreases from 784 nodes, to 128 nodes, to 64 nodes and then to 10 nodes. \n",
        "\n",
        "<p align=\"center\"><img src=\"https://mlfromscratch.com/content/images/2020/02/deep_nn-1.png\" width=\"60%\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH4fhFq2X96E",
        "colab_type": "text"
      },
      "source": [
        "### **Imports And Dataset**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yJxKRMxYCga",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7f501f2-34a1-4899-95aa-0473fba722d1"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqSnmb-PYJra",
        "colab_type": "text"
      },
      "source": [
        "Now we have to load the dataset and preprocess it, so that we can use it in NumPy. We do normalization by dividing all images by 255, and make it such that all images have values between 0 and 1, since this removes some of the numerical stability issues with activation functions later on. We choose to go with one-hot encoded labels, since we can more easily subtract these labels from the output of the neural network. We also choose to load our inputs as flattened arrays of 28 * 28 = 784 elements, since that is what the input layer requires."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyPRsLjhYCd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = fetch_openml('mnist_784', version=1, return_X_y=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbSNMF0cYrM5",
        "colab_type": "text"
      },
      "source": [
        "### **Example** : \n",
        "\n",
        "```python\n",
        "In [1]: from keras.utils import np_utils\n",
        "\n",
        "In [2]: y_train = [1, 0, 3, 4, 5, 0, 2, 1]\n",
        "\n",
        "In [3]: \"\"\" Assuming the labeled dataset has total six classes (0 to 5), y_train is the true label array \"\"\"\n",
        "\n",
        "In [4]: np_utils.to_categorical(y_train, num_classes=6)\n",
        "Out[4]:\n",
        "array([[ 0.,  1.,  0.,  0.,  0.,  0.],\n",
        "       [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0.,  1.,  0.,  0.],\n",
        "       [ 0.,  0.,  0.,  0.,  1.,  0.],\n",
        "       [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
        "       [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  1.,  0.,  0.,  0.],\n",
        "       [ 0.,  1.,  0.,  0.,  0.,  0.]])\n",
        "````\n",
        "\n",
        "Reference :\n",
        "\n",
        "* [https://stackoverflow.com/questions/41494625/issues-using-keras-np-utils-to-categorical](https://stackoverflow.com/questions/41494625/issues-using-keras-np-utils-to-categorical)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH9RiAjQYCaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = (x/255).astype('float32')\n",
        "y = to_categorical(y)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPhaGno8Ynq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbyRM-0xZO1c",
        "colab_type": "text"
      },
      "source": [
        "### **Initialization**\n",
        "\n",
        "The initialization of weights in the neural network is kind of hard to think about. To really understand how and why the following approach works, you need a grasp of linear algebra, specifically dimensionality when using the dot product operation.\n",
        "\n",
        "The specific problem that arises, when trying to implement the feedforward neural network, is that we are trying to transform from 784 nodes all the way down to 10 nodes. When instantiating the DeepNeuralNetwork class, we pass in an array of sizes that defines the number of activations for each layer.\n",
        "\n",
        "Each iteration of the training process consists of the following steps:\n",
        "\n",
        "* Calculating the predicted output ŷ, known as feedforward\n",
        "* Updating the weights and biases, known as backpropagation\n",
        "\n",
        "The sequential graph below illustrates the process.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://miro.medium.com/max/1400/1*CEtt0h8Rss_qPu7CyqMTdQ.png\" width=\"60%\"/>\n",
        "\n",
        "\n",
        "```python\n",
        "dnn = DeepNeuralNetwork(sizes=[784, 128, 64, 10])\n",
        "```\n",
        "This initializes the DeepNeuralNetwork class by the init function.\n",
        "\n",
        "```python\n",
        "def __init__(self, sizes, epochs=10, l_rate=0.001):\n",
        "    self.sizes = sizes\n",
        "    self.epochs = epochs\n",
        "    self.l_rate = l_rate\n",
        "\n",
        "    # we save all parameters in the neural network in this dictionary\n",
        "    self.params = self.initialization()\n",
        "```\n",
        "We can only use the dot product operation for two matrices M1 and M2, where $m$ in M1 is equal to $n$ in M2, or where $n$ in M1 is equal to $m$ in M2.\n",
        "\n",
        "With this explanation, you can see that we initialize the first set of weights W1 with $m$ = 128 and $n$ = 784 , while the next weights W2 are $m$ = 64 and \n",
        "$n$ = 128.\n",
        "\n",
        "The number of activations in the input layer A0 is equal to 784, as explained earlier, and when we dot W1 by the activations A0, the operation is successful.\n",
        "\n",
        "```python\n",
        "def initialization(self):\n",
        "    # number of nodes in each layer\n",
        "    input_layer=self.sizes[0]\n",
        "    hidden_1=self.sizes[1]\n",
        "    hidden_2=self.sizes[2]\n",
        "    output_layer=self.sizes[3]\n",
        "\n",
        "    params = {\n",
        "        'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
        "        'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
        "        'W3':np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
        "    }\n",
        "\n",
        "    return params\n",
        "```\n",
        "\n",
        "### **Feedforward** <br/>\n",
        "\n",
        "The forward pass consists of the dot operation in NumPy, which turns out to be just matrix multiplication. As described in the introduction to neural networks article, we have to multiply the weights by the activations of the previous layer. Then we have to apply the activation function to the outcome.\n",
        "\n",
        "To get through each layer, we sequentially apply the dot operation, followed by the sigmoid activation function. In the last layer we use the softmax activation function, since we wish to have probabilities of each class, so that we can measure how well our current forward pass performs.\n",
        "\n",
        "Note: A numerical stable version of the softmax function was chosen, you can read more from the course at [Stanford called CS231n](http://cs231n.github.io/linear-classify/#softmax).\n",
        "\n",
        "```python\n",
        "def forward_pass(self, x_train):\n",
        "    params = self.params\n",
        "\n",
        "    # input layer activations becomes sample\n",
        "    params['A0'] = x_train\n",
        "\n",
        "    # input layer to hidden layer 1\n",
        "    params['Z1'] = np.dot(params[\"W1\"], params['A0'])\n",
        "    params['A1'] = self.sigmoid(params['Z1'])\n",
        "\n",
        "    # hidden layer 1 to hidden layer 2\n",
        "    params['Z2'] = np.dot(params[\"W2\"], params['A1'])\n",
        "    params['A2'] = self.sigmoid(params['Z2'])\n",
        "\n",
        "    # hidden layer 2 to output layer\n",
        "    params['Z3'] = np.dot(params[\"W3\"], params['A2'])\n",
        "    params['A3'] = self.softmax(params['Z3'])\n",
        "\n",
        "    return params['A3']\n",
        "```\n",
        "\n",
        "The following are the activation functions used for this article. As can be observed, we provide a derivative version of the sigmoid, since we will need that later on when backpropagating through the neural network.\n",
        "\n",
        "```python\n",
        "def sigmoid(self, x, derivative=False):\n",
        "    if derivative:\n",
        "        return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "def softmax(self, x):\n",
        "    # Numerically stable with large exponentials\n",
        "    exps = np.exp(x - x.max())\n",
        "    return exps / np.sum(exps, axis=0)\n",
        "```\n",
        "\n",
        "### **Loss Function**\n",
        "\n",
        "There are many available loss functions, and the nature of our problem should dictate our choice of loss function. In this tutorial, we’ll use a simple sum-of-sqaures error as our loss function.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://miro.medium.com/max/600/1*iNa1VLdaeqwUAxpNXs3jwQ.png\"/>\n",
        "\n",
        "That is, the sum-of-squares error is simply the sum of the difference between each predicted value and the actual value. The difference is squared so that we measure the absolute value of the difference.\n",
        "\n",
        "Our goal in training is to find the best set of weights and biases that minimizes the loss function.\n",
        "\n",
        "### **Backpropagation**\n",
        "\n",
        "Now that we’ve measured the error of our prediction (loss), we need to find a way to propagate the error back, and to update our weights and biases.\n",
        "In order to know the appropriate amount to adjust the weights and biases by, we need to know the derivative of the loss function with respect to the weights and biases.\n",
        "Recall from calculus that the derivative of a function is simply the slope of the function.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://miro.medium.com/max/1400/1*3FgDOt4kJxK2QZlb9T0cpg.png\" width=\"60%\"/>\n",
        "\n",
        "If we have the derivative, we can simply update the weights and biases by increasing/reducing with it(refer to the diagram above). This is known as gradient descent.\n",
        "\n",
        "However, we can’t directly calculate the derivative of the loss function with respect to the weights and biases because the equation of the loss function does not contain the weights and biases. Therefore, we need the chain rule to help us calculate it.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://miro.medium.com/max/1400/1*7zxb2lfWWKaVxnmq2o69Mw.png\" width=\"60%\"/>\n",
        "\n",
        " the derivative (slope) of the loss function with respect to the weights, so that we can adjust the weights accordingly.\n",
        " \n",
        "The backward pass is hard to get right, because there are so many sizes and operations that have to align, for all the operations to be successful. Here is the full function for the backward pass; we will go through each weight update below.\n",
        "\n",
        "```python\n",
        "def backward_pass(self, y_train, output):\n",
        "    '''\n",
        "        This is the backpropagation algorithm, for calculating the updates\n",
        "        of the neural network's parameters.\n",
        "    '''\n",
        "    params = self.params\n",
        "    change_w = {}\n",
        "\n",
        "    # Calculate W3 update\n",
        "    error = output - y_train\n",
        "    change_w['W3'] = np.dot(error, params['A3'])\n",
        "\n",
        "    # Calculate W2 update\n",
        "    error = np.multiply( np.dot(params['W3'].T, error), self.sigmoid(params['Z2'], derivative=True) )\n",
        "    change_w['W2'] = np.dot(error, params['A2'])\n",
        "\n",
        "    # Calculate W1 update\n",
        "    error = np.multiply( np.dot(params['W2'].T, error), self.sigmoid(params['Z1'], derivative=True) )\n",
        "    change_w['W1'] = np.dot(error, params['A1'])\n",
        "\n",
        "    return change_w\n",
        "```    \n",
        "\n",
        "**W3 Update** <br/>\n",
        "\n",
        "The update for W3 can be calculated by subtracting the ground truth array with labels called y_train from the output of the forward pass called output. This operation is successful, because len(y_train) is 10 and len(output) is also 10.\n",
        "\n",
        "An example of y_train might be the following, where the 1 is corresponding to the label of the output:\n",
        "```python\n",
        "y_train = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
        "```\n",
        "While an example of output might be the following, where the numbers are probabilities corresponding to the classes of y_train:\n",
        "\n",
        "```python\n",
        "output = np.array([0.2, 0.2, 0.5, 0.3, 0.6, 0.4, 0.2, 0.1, 0.3, 0.7])\n",
        "```\n",
        "If we subtract them, we get the following:\n",
        "```python\n",
        ">>> output - y_train\n",
        "array([ 0.2,  0.2, -0.5,  0.3,  0.6,  0.4,  0.2,  0.1,  0.3,  0.7])\n",
        "```\n",
        "\n",
        "The next operation is the dot operation that dots the error (which we just calculated) with the activations of the last layer.\n",
        "\n",
        "```python\n",
        "error = output - y_train\n",
        "change_w['W3'] = np.dot(error, params['A3'])\n",
        "```\n",
        "### **W2 Update**\n",
        "\n",
        "The next is updating the weights W2. More operations are involved for success. Firstly, there is a slight mismatch in shapes, because W3 has the shape (10, 64), and error has (10, 64), i.e. the exact same dimensions. Thus, we can use a transpose operation on the W3 parameter by the .T, such that the array has its dimensions permuted and the shapes now align up for the dot operation.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://mlfromscratch.com/content/images/2020/03/transpose.png\" width=\"40%\"/>\n",
        "\n",
        "W3 now has shape (64, 10) and error has shape (10, 64), which are compatible with the dot operation. The result is multiplied element-wise (also called Hadamard product) with the outcome of the derivative of the sigmoid function of Z2. At last, we dot the error with the activations of the previous layer.\n",
        "\n",
        "```python\n",
        "error = np.multiply( np.dot(params['W3'].T, error), self.sigmoid(params['Z2'], derivative=True) )\n",
        "change_w['W2'] = np.dot(error, params['A2'])\n",
        "```\n",
        "\n",
        "### **W1 Update**\n",
        "\n",
        "Likewise, the code for updating W1 is using the parameters of the neural network one step earlier. Except for other parameters, the code is equivalent to the W2 update.\n",
        "```python\n",
        "error = np.multiply( np.dot(params['W2'].T, error), self.sigmoid(params['Z1'], derivative=True) )\n",
        "change_w['W1'] = np.dot(error, params['A1'])\n",
        "```\n",
        "\n",
        "### **Training (Stochastic Gradient Descent)**\n",
        "\n",
        "We have defined a forward and backward pass, but how can we start using them? We have to make a training loop and choose to use Stochastic Gradient Descent (SGD) as the optimizer to update the parameters of the neural network.\n",
        "\n",
        "There are two main loops in the training function. One loop for the number of epochs, which is the number of times we run through the whole dataset, and a second loop for running through each observation one by one.\n",
        "\n",
        "For each observation, we do a forward pass with x, which is one image in an array with the length 784, as explained earlier. The output of the forward pass is used along with y, which are the one-hot encoded labels (the ground truth), in the backward pass. This gives us a dictionary of updates to the weights in the neural network.\n",
        "\n",
        "```python\n",
        "def train(self, x_train, y_train, x_val, y_val):\n",
        "    start_time = time.time()\n",
        "    for iteration in range(self.epochs):\n",
        "        for x,y in zip(x_train, y_train):\n",
        "            output = self.forward_pass(x)\n",
        "            changes_to_w = self.backward_pass(y, output)\n",
        "            self.update_network_parameters(changes_to_w)\n",
        "        \n",
        "        accuracy = self.compute_accuracy(x_val, y_val)\n",
        "        print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2}'.format(\n",
        "            iteration+1, time.time() - start_time, accuracy\n",
        "        ))\n",
        "```\n",
        "\n",
        "The **update_network_parameters()** function has the code for the SGD update rule, which just needs the gradients for the weights as input. And to be clear, SGD involves calculating the gradient using backpropagation from the backward pass, not just updating the parameters. They seem separate and they should be thought of separately, since the two algorithms are different.\n",
        "\n",
        "```python\n",
        "def update_network_parameters(self, changes_to_w):\n",
        "    '''\n",
        "        Update network parameters according to update rule from\n",
        "        Stochastic Gradient Descent.\n",
        "\n",
        "        θ = θ - η * ∇J(x, y), \n",
        "            theta θ:            a network parameter (e.g. a weight w)\n",
        "            eta η:              the learning rate\n",
        "            gradient ∇J(x, y):  the gradient of the objective function,\n",
        "                                i.e. the change for a specific theta θ\n",
        "    '''\n",
        "    \n",
        "    for key, value in changes_to_w.items():\n",
        "        for w_arr in self.params[key]:\n",
        "            w_arr -= self.l_rate * value\n",
        "```\n",
        "\n",
        "After having updated the parameters of the neural network, we can measure the accuracy on a validation set that we conveniently prepared earlier, to validate how well our network performs after each iteration over the whole dataset.\n",
        "\n",
        "This code uses some of the same pieces as the training function; to begin with, it does a forward pass, then it finds the prediction of the network and checks for equality with the label. After that, we sum over the predictions and divide by 100 to find the accuracy, and at last, we average out the accuracy of each class.\n",
        "\n",
        "```python\n",
        "def compute_accuracy(self, x_val, y_val):\n",
        "    '''\n",
        "        This function does a forward pass of x, then checks if the indices\n",
        "        of the maximum value in the output equals the indices in the label\n",
        "        y. Then it sums over each prediction and calculates the accuracy.\n",
        "    '''\n",
        "    predictions = []\n",
        "\n",
        "    for x, y in zip(x_val, y_val):\n",
        "        output = self.forward_pass(x)\n",
        "        pred = np.argmax(output)\n",
        "        predictions.append(pred == y)\n",
        "    \n",
        "    summed = sum(pred for pred in predictions) / 100.0\n",
        "    return np.average(summed)\n",
        "```\n",
        "\n",
        "Finally, we can call the training function, after knowing what will happen. We use the training and validation data as input to the training function, and then we wait.\n",
        "\n",
        "dnn.train(x_train, y_train, x_val, y_val)\n",
        "Note that the results may vary a lot, depending on how the weights are initialized. My results range from an accuracy of 0%, and all the way to 95%.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w4nQT_8eP8u",
        "colab_type": "text"
      },
      "source": [
        "### **References**:\n",
        "\n",
        "* [Neural-network-tutorial by mlfromscratch ](https://mlfromscratch.com/neural-network-tutorial/)\n",
        "\n",
        "* [How to build are own neural network - Medium](https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6)\n",
        "\n",
        "* [Building-neural-networks-from-scratch-with-python-code-and-math-in-detail](https://medium.com/towards-artificial-intelligence/building-neural-networks-from-scratch-with-python-code-and-math-in-detail-i-536fae5d7bbf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuUZxA06YCW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeepNeuralNetwork():\n",
        "    def __init__(self, sizes, epochs=10, l_rate=0.001):\n",
        "        self.sizes = sizes\n",
        "        self.epochs = epochs\n",
        "        self.l_rate = l_rate\n",
        "\n",
        "        # we save all parameters in the neural network in this dictionary\n",
        "        self.params = self.initialization()\n",
        "\n",
        "    def sigmoid(self, x, derivative=False):\n",
        "        if derivative:\n",
        "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "        return 1/(1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        # Numerically stable with large exponentials\n",
        "        exps = np.exp(x - x.max())\n",
        "        return exps / np.sum(exps, axis=0)\n",
        "\n",
        "    def initialization(self):\n",
        "        # number of nodes in each layer\n",
        "        input_layer=self.sizes[0]\n",
        "        hidden_1=self.sizes[1]\n",
        "        hidden_2=self.sizes[2]\n",
        "        output_layer=self.sizes[3]\n",
        "\n",
        "        params = {\n",
        "            'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
        "            'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
        "            'W3':np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
        "        }\n",
        "\n",
        "        return params\n",
        "\n",
        "    def forward_pass(self, x_train):\n",
        "        params = self.params\n",
        "\n",
        "        # input layer activations becomes sample\n",
        "        params['A0'] = x_train\n",
        "\n",
        "        # input layer to hidden layer 1\n",
        "        params['Z1'] = np.dot(params[\"W1\"], params['A0'])\n",
        "        params['A1'] = self.sigmoid(params['Z1'])\n",
        "\n",
        "        # hidden layer 1 to hidden layer 2\n",
        "        params['Z2'] = np.dot(params[\"W2\"], params['A1'])\n",
        "        params['A2'] = self.sigmoid(params['Z2'])\n",
        "\n",
        "        # hidden layer 2 to output layer\n",
        "        params['Z3'] = np.dot(params[\"W3\"], params['A2'])\n",
        "        params['A3'] = self.softmax(params['Z3'])\n",
        "\n",
        "        return params['A3']\n",
        "\n",
        "    def backward_pass(self, y_train, output):\n",
        "        '''\n",
        "            This is the backpropagation algorithm, for calculating the updates\n",
        "            of the neural network's parameters.\n",
        "\n",
        "            Note: There is a stability issue that causes warnings. This is \n",
        "                  caused  by the dot and multiply operations on the huge arrays.\n",
        "                  \n",
        "                  RuntimeWarning: invalid value encountered in true_divide\n",
        "                  RuntimeWarning: overflow encountered in exp\n",
        "                  RuntimeWarning: overflow encountered in square\n",
        "        '''\n",
        "        params = self.params\n",
        "        change_w = {}\n",
        "\n",
        "        # Calculate W3 update\n",
        "        error = output - y_train\n",
        "        change_w['W3'] = np.dot(error, params['A3'])\n",
        "\n",
        "        # Calculate W2 update\n",
        "        error = np.multiply( np.dot(params['W3'].T, error), self.sigmoid(params['Z2'], derivative=True) )\n",
        "        change_w['W2'] = np.dot(error, params['A2'])\n",
        "\n",
        "        # Calculate W1 update\n",
        "        error = np.multiply( np.dot(params['W2'].T, error), self.sigmoid(params['Z1'], derivative=True) )\n",
        "        change_w['W1'] = np.dot(error, params['A1'])\n",
        "\n",
        "        return change_w\n",
        "\n",
        "    def update_network_parameters(self, changes_to_w):\n",
        "        '''\n",
        "            Update network parameters according to update rule from\n",
        "            Stochastic Gradient Descent.\n",
        "\n",
        "            θ = θ - η * ∇J(x, y), \n",
        "                theta θ:            a network parameter (e.g. a weight w)\n",
        "                eta η:              the learning rate\n",
        "                gradient ∇J(x, y):  the gradient of the objective function,\n",
        "                                    i.e. the change for a specific theta θ\n",
        "        '''\n",
        "        \n",
        "        for key, value in changes_to_w.items():\n",
        "            for w_arr in self.params[key]:\n",
        "                w_arr -= self.l_rate * value\n",
        "\n",
        "    def compute_accuracy(self, x_val, y_val):\n",
        "        '''\n",
        "            This function does a forward pass of x, then checks if the indices\n",
        "            of the maximum value in the output equals the indices in the label\n",
        "            y. Then it sums over each prediction and calculates the accuracy.\n",
        "        '''\n",
        "        predictions = []\n",
        "\n",
        "        for x, y in zip(x_val, y_val):\n",
        "            output = self.forward_pass(x)\n",
        "            pred = np.argmax(output)\n",
        "            predictions.append(pred == y)\n",
        "        \n",
        "        summed = sum(pred for pred in predictions) / 100.0\n",
        "        return np.average(summed)\n",
        "\n",
        "    def train(self, x_train, y_train, x_val, y_val):\n",
        "        start_time = time.time()\n",
        "        for iteration in range(self.epochs):\n",
        "            for x,y in zip(x_train, y_train):\n",
        "                output = self.forward_pass(x)\n",
        "                changes_to_w = self.backward_pass(y, output)\n",
        "                self.update_network_parameters(changes_to_w)\n",
        "            \n",
        "            accuracy = self.compute_accuracy(x_val, y_val)\n",
        "            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2}'.format(\n",
        "                iteration+1, time.time() - start_time, accuracy\n",
        "            ))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkYynGc0Byzl",
        "colab_type": "text"
      },
      "source": [
        "### Results\n",
        "\n",
        "Completely dependent on how the weights are initialized, we get different results. Sometimes we are stuck at 0% accuracy, sometimes 5-10%, other times it jumps from 22% to 94.5%. If you want to experiment, try using a seed for numpy by `np.random.seed(42)` or any other number. Then you should get the same results each time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsg-fGC8Phyb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "906bef84-9311-4e06-ba68-a56a36c2ffc2"
      },
      "source": [
        "dnn = DeepNeuralNetwork(sizes=[784, 128, 64, 10])\n",
        "dnn.train(x_train, y_train, x_val, y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Time Spent: 74.69s, Accuracy: 5.8\n",
            "Epoch: 2, Time Spent: 149.62s, Accuracy: 6.464\n",
            "Epoch: 3, Time Spent: 224.60s, Accuracy: 6.833\n",
            "Epoch: 4, Time Spent: 298.91s, Accuracy: 7.0969999999999995\n",
            "Epoch: 5, Time Spent: 373.80s, Accuracy: 7.272999999999999\n",
            "Epoch: 6, Time Spent: 447.39s, Accuracy: 7.394\n",
            "Epoch: 7, Time Spent: 520.99s, Accuracy: 7.492\n",
            "Epoch: 8, Time Spent: 594.71s, Accuracy: 7.587999999999999\n",
            "Epoch: 9, Time Spent: 668.23s, Accuracy: 7.671000000000001\n",
            "Epoch: 10, Time Spent: 742.01s, Accuracy: 7.758\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}