{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_ML_L6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blXAIWH0h5QX",
        "colab_type": "text"
      },
      "source": [
        "# Assignment on Feature Engineering (L6)\n",
        "\n",
        "This assignment has been adapted from the course **Feature Engineering for Machine Learning in Python** On DataCamp. \n",
        "\n",
        "We will explore what feature engineering is and how to get started with applying it to real-world data. We will be working with a modified subset of the [Stackoverflow survey response data](https://insights.stackoverflow.com/survey/2018/#overview). This data set records the details, and preferences of thousands of users of the StackOverflow website. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8OJHHSFhkVo",
        "colab_type": "text"
      },
      "source": [
        "## Creating Features \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxrLsTxvi4Ef",
        "colab_type": "code",
        "outputId": "91c8aa61-494d-4d23-e175-91edf1d056a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw55w5YsB6jE",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Inspect your data\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCpuZS41h35f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the data which has been stored as a CSV on the URL given below:\n",
        "so_survey_csv = 'https://assets.datacamp.com/production/repositories/3752/datasets/19699a2441073ad6459bf5e3e17690e2cae86cf1/Combined_DS_v10.csv'\n",
        "\n",
        "# Import so_survey_csv into so_survey_df\n",
        "so_survey_df = pd.read_csv(so_survey_csv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO7TR7EGBgDP",
        "colab_type": "text"
      },
      "source": [
        "Instructions: For the `so_survey_df` DataFrame, \n",
        "* Print its shape and its first five rows. \n",
        "* Print the data type of each column.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MzXp3T8Cl8W",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Selecting specific data types\n",
        "---\n",
        "Often a data set will contain columns with several different data types (like the one we are working with). The majority of machine learning models require us to have a consistent data type across features. Similarly, most feature engineering techniques are applicable to only one type of data at a time. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glnEQlrVCobL",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Create a subset of `so_survey_df` consisting of only the numeric (int and float) columns and save it as `so_numeric_df`. \n",
        "* Print the column names contained in `so_numeric_df`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_4kpncQDXuj",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### One-hot encoding and dummy variables\n",
        "---\n",
        "To use categorical variables in a machine learning model, we first need to represent them in a quantitative way. The two most common approaches are to one-hot encode the variables using or to use dummy variables. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRQM6bVaDhnT",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* One-hot encode the `Country` column of `so_survey_df` DataFrame, adding \"OH\" as a prefix for each column.\n",
        "* Create dummy variables for the `Country` column, adding \"DM\" as a prefix for each column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ56-6j9oz4H",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Dealing with uncommon categories\n",
        "---\n",
        "Some features can have many different categories but a very uneven distribution of their occurrences. Take for example Data Science's favorite languages to code in, some common choices are Python, R, and Julia, but there can be individuals with bespoke choices, like FORTRAN, C etc. In these cases, we may not want to create a feature for each value, but only the more common occurrences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z1TCKQ7EHcb",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Extract the `Country` column of `so_survey_df` as a series and assign it to `countries`.\n",
        "* Find the counts of each category in the newly created `countries` series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKxW1Z70EW5W",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Create a mask for values occurring less than 10 times in `country_counts`.\n",
        "* Print the first 5 rows of the mask."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zro-imOEiwt",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Label values occurring less than the `mask` cutoff as 'Other'.\n",
        "* Print the new category counts in `countries`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0j7IlzLpke5",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Binarizing columns\n",
        "---\n",
        "While numeric values can often be used without any feature engineering, there will be cases when some form of manipulation can be useful. For example on some occasions, we might not care about the magnitude of a value but only care about its direction, or if it exists at all. In these situations, we will want to binarize a column. In the `so_survey_df` data, we have a large number of survey respondents that are working voluntarily (without pay). We will create a new column titled `Paid_Job` indicating whether each person is paid (their salary is greater than zero)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdCPaTzFExzS",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Create a new column called `Paid_Job` filled with zeros.\n",
        "* Replace all the `Paid_Job` values with a 1 where the corresponding `ConvertedSalary` is greater than 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lJTmpvzq_NJ",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Binning values\n",
        "---\n",
        "For many continuous values we will care less about the exact value of a numeric column, but instead care about the bucket it falls into. This can be useful when plotting values, or simplifying your machine learning models. It is mostly used on continuous variables where accuracy is not the biggest concern e.g. age, height, wages.\n",
        "\n",
        "Bins are created using `pd.cut(df['column_name'], bins)` where bins can be an integer specifying the number of evenly spaced bins, or a list of bin boundaries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GglGS3NkFtU3",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Bin the value of the `ConvertedSalary` column in `so_survey_df` into 5 equal bins, in a new column called `equal_binned`.\n",
        "* Print the first five rows of both columns: `ConvertedSalary` and `equal_binned`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o_1_UiiF1dc",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Bin the `ConvertedSalary` column using the boundaries in the list bins and label the bins using `labels` in a new column called `boundary_binned`. \n",
        "* Print the first 5 rows of the `boundary_binned` column. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZyNvV6krVLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify the boundaries of the bins\n",
        "bins = [-np.inf, 10000, 50000, 100000, 150000, np.inf]\n",
        "\n",
        "# Bin labels\n",
        "labels = ['Very low', 'Low', 'Medium', 'High', 'Very high']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIkFDAVUGhcQ",
        "colab_type": "text"
      },
      "source": [
        "## Dealing with Messy Data\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8o-Z4YEsYF9",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### How sparse is my data?\n",
        "---\n",
        "Most data sets contain missing values, often represented as NaN (Not a Number). If we are working with Pandas, we can easily check how many missing values exist in each column.\n",
        "\n",
        "Let's find out how many of the developers taking the survey chose to enter their age (found in the `Age` column of `so_survey_df`) and their gender (`Gender` column of `so_survey_df`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n0JDxzMHMwD",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Subset the `so_survey_df` DataFrame to only include the `Age` and `Gender` columns.\n",
        "* Print the number of non-missing values in both columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STDb03R3swA4",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Finding the missing values\n",
        "---\n",
        "While having a summary of how much of your data is missing can be useful, often we will need to find the exact locations of these missing values. Using the same subset of the StackOverflow data from the last exercise (`sub_df`), we will show how a value can be flagged as missing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5XbMPSrHhY4",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Print the first 10 entries of the `sub_df` DataFrame.\n",
        "* Print the locations of the missing values in the first 10 rows of this DataFrame.\n",
        "* Print the locations of the non-missing values in the first 10 rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgR_uodEtS2o",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Listwise deletion\n",
        "---\n",
        "The simplest way to deal with missing values in our dataset when they are occurring entirely at random is to remove those rows, also called 'listwise deletion'.\n",
        "\n",
        "Depending on the use case, we will sometimes want to remove all missing values in our data while other times we may want to only remove a particular column if too many values are missing in that column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIioVg70H8fY",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Print the number of rows and columns in `so_survey_df`.\n",
        "* Drop all rows with missing values in `so_survey_df`.\n",
        "* Drop all columns with missing values in `so_survey_df`.\n",
        "* Drop all rows in `so_survey_df` where `Gender` is missing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C1H8JwhtxHU",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Replacing missing values with constants\n",
        "---\n",
        "While removing missing data entirely maybe a correct approach in many situations, this may result in a lot of information being omitted from your models.\n",
        "\n",
        "We may find categorical columns where the missing value is a valid piece of information in itself, such as someone refusing to answer a question in a survey. In these cases, we can fill all missing values with a new category entirely, for example 'No response given'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPq_Ae40IUnY",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Print the count of occurrences of each category in `so_survey_df`'s `Gender` column.\n",
        "* Replace all missing values in the `Gender` column with the string 'Not Given'. Make changes to the original DataFrame.\n",
        "* Print the count of occurrences of updated category in `so_survey_df`'s `Gender` column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn2h-F8IuI0C",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Filling continuous missing values\n",
        "---\n",
        "Earlier, we dealt with different methods of removing data missing values and filling in missing values with a fixed string. These approaches are valid in many cases, particularly when dealing with categorical columns but have limited use when working with continuous values. In these cases, it may be most valid to fill the missing values in the column with a value calculated from the entries present in the column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S9YZ2GsIyLx",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Print the first five rows of the `StackOverflowJobsRecommend` column of `so_survey_df`.\n",
        "* Replace the missing values in the `StackOverflowJobsRecommend` column with its mean. Make changes directly to the original DataFrame.\n",
        "* Round the decimal values that we introduced in the `StackOverflowJobsRecommend` column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKcC12f0vCuh",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Dealing with stray characters (I)\n",
        "---\n",
        "In this exercise, we will work with the `RawSalary` column of so_survey_df which contains the wages of the respondents along with the currency symbols and commas, such as $42,000. When importing data from Microsoft Excel, more often that not we will come across data in this form."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz0CGSQkJI03",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Remove the commas (,) from the `RawSalary` column.\n",
        "* Remove the dollar ($) signs from the `RawSalary` column.\n",
        "* Print the first five rows of updated `RawSalary` column. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpayqY5IwMBl",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Dealing with stray characters (II)\n",
        "---\n",
        "In the last exercise, we could tell quickly based off of the `df.head()` call which characters were causing an issue. In many cases this will not be so apparent. There will often be values deep within a column that are preventing us from casting a column as a numeric type so that it can be used in a model or further feature engineering.\n",
        "\n",
        "One approach to finding these values is to force the column to the data type desired using `pd.to_numeric()`, coercing any values causing issues to `NaN`, Then filtering the DataFrame by just the rows containing the `NaN` values.\n",
        "\n",
        "Try to cast the `RawSalary` column as a float and it will fail as an additional character can now be found in it. Find the character and remove it so the column can be cast as a float."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7Duf4v7JpPv",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Attempt to convert the `RawSalary` column of `so_survey_df` to numeric values coercing all failures into null values.\n",
        "* Find the indexes of the rows containing `NaN`s.\n",
        "* Print the rows in `RawSalary` based on these indexes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYuh53vbv5_d",
        "colab_type": "code",
        "outputId": "7f02eefc-b7da-485b-c391-dcab68a352c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "# Attempt to convert the column to numeric values\n",
        "numeric_vals = pd.to_numeric(so_survey_df['RawSalary'], errors='coerce')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0            NaN\n",
            "2            NaN\n",
            "4      £41671.00\n",
            "6            NaN\n",
            "8            NaN\n",
            "         ...    \n",
            "989          NaN\n",
            "990          NaN\n",
            "992          NaN\n",
            "994          NaN\n",
            "997          NaN\n",
            "Name: RawSalary, Length: 401, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyllnJ5NKEwy",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Did you notice the pound (£) signs in the `RawSalary` column? Remove these signs like we did in the previous exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feXhsa94wtBz",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Method chaining\n",
        "---\n",
        "When applying multiple operations on the same column (like in the previous exercises), you made the changes in several steps, assigning the results back in each step. However, when applying multiple successive operations on the same column, you can \"chain\" these operations together for clarity and ease of management. This can be achieved by calling multiple methods sequentially:\n",
        "```\n",
        "# Method chaining\n",
        "df['column'] = df['column'].method1().method2().method3()\n",
        "\n",
        "# Same as \n",
        "df['column'] = df['column'].method1()\n",
        "df['column'] = df['column'].method2()\n",
        "df['column'] = df['column'].method3()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo-JUG77KTRi",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Remove the commas (`,`) from the `RawSalary` column of `so_survey_df`.\n",
        "* Remove the dollar (`$`) signs from the `RawSalary` column.\n",
        "* Remove the pound (`£`) signs from the `RawSalary` column.\n",
        "* Convert the `RawSalary` column to float."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqpCNv72Kl_1",
        "colab_type": "text"
      },
      "source": [
        "## Conforming to Statistical Assumptions\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYwlN2FyyBml",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### What does your data look like?\n",
        "---\n",
        "Up until now we have focused on creating new features and dealing with issues in our data. Feature engineering can also be used to make the most out of the data that we already have and use it more effectively when creating machine learning models.\n",
        "Many algorithms may assume that our data is normally distributed, or at least that all our columns are on the same scale. This will often not be the case, e.g. one feature may be measured in thousands of dollars while another would be number of years. In this exercise, we will create plots to examine the distributions of some numeric columns in the `so_survey_df` DataFrame. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHMvrvwxyTQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "so_numeric_df = so_survey_df[['ConvertedSalary', 'Age', 'Years Experience']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8Ch5slQy6B-",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Generate a histogram of all columns in the `so_numeric_df` DataFrame.\n",
        "* Generate box plots of the `Age` and `Years Experience` columns in the `so_numeric_df` DataFrame.\n",
        "* Generate a box plot of the `ConvertedSalary` column in the `so_numeric_df`.\n",
        "* Plot pairwise relationships (using `sns.pairplot`) in the `so_numeric_df`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op7uFM-70CW5",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Normalization\n",
        "---\n",
        "In normalization we linearly scale the entire column between 0 and 1, with 0 corresponding with the lowest value in the column, and 1 with the largest. When using scikit-learn (the most commonly used machine learning library in Python) we can use a `MinMaxScaler` to apply normalization. (It is called this as it scales our values between a minimum and maximum value.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME9djjbu0K83",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Fit the `MinMaxScaler` on the `Age` column of `so_numeric_df`.\n",
        "* Transform the same column with the scaler you just fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNgwBJRp0Zgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import MinMaxScaler\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYeEJLVQ0HCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate MinMaxScaler\n",
        "MM_scaler = MinMaxScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBKJrNv60m2p",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Standardization\n",
        "---\n",
        "While normalization can be useful for scaling a column between two data points, it is hard to compare two scaled columns if even one of them is overly affected by outliers. One commonly used solution to this is called standardization, where instead of having a strict upper and lower bound, you center the data around its mean, and calculate the number of standard deviations away from mean each data point is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFyXLW3v0rU9",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Fit the `StandardScaler` on the `Age` column of `so_numeric_df`.\n",
        "* Transform the same column with the scaler we just fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL8N_V9n03jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOoRs_vR0qmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate StandardScaler\n",
        "SS_scaler = StandardScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLntVWcp1aZ1",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Log transformation\n",
        "---\n",
        "In the previous exercises we scaled the data linearly, which will not affect the data's shape. This works great if our data is normally distributed (or closely normally distributed), an assumption that a lot of machine learning models make. Sometimes we will work with data that closely conforms to normality, e.g the height or weight of a population. On the other hand, many variables in the real world do not follow this pattern e.g, wages or age of a population. \n",
        "\n",
        "Now, we will use a log transform on the `ConvertedSalary` column in the `so_numeric_df` DataFrame as it has a large amount of its data centered around the lower values, but contains very high values also. These distributions are said to have a long right tail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMQB-F4e1lMK",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Fit the `PowerTransformer` on the `ConvertedSalary` column of `so_numeric_df`.\n",
        "* Transform the same column with the scaler we just fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N95T8tjM1h5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import PowerTransformer\n",
        "from sklearn.preprocessing import PowerTransformer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CNzVVOg1ueY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate PowerTransformer\n",
        "pow_trans = PowerTransformer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhlDOrtb18uG",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Percentage based outlier removal\n",
        "---\n",
        "One way to ensure a small portion of data is not having an overly adverse effect is by removing a certain percentage of the largest and/or smallest values in the column. This can be achieved by finding the relevant quantile and trimming the data using it with a mask. This approach is particularly useful if we are concerned that the highest values in our dataset should be avoided. When using this approach, we must remember that even if there are no outliers, this will still remove the same top N percentage from the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhiOqx-v2CFB",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Find the 95th quantile of the `ConvertedSalary` column in `so_numeric_df` DataFrame.\n",
        "* Trim the `so_numeric_df` DataFrame to retain all rows where `ConvertedSalary` is less than it's 95th quantile and store this as `trimmed_df`.\n",
        "* Plot the histogram of `so_numeric_df[['ConvertedSalary']]`.\n",
        "* Plot the histogram of `trimmed_df[['ConvertedSalary']]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1_-eGVu1_JA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the 95th quantile\n",
        "quantile = so_numeric_df['ConvertedSalary'].quantile(0.95)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgva617FNqZy",
        "colab_type": "text"
      },
      "source": [
        "## Dealing with Text Data\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuHUqVR03PJV",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Cleaning up your text\n",
        "---\n",
        "Unstructured text data cannot be directly used in most analyses. Multiple steps need to be taken to go from a long free form string to a set of numeric columns in the right format that can be ingested by a machine learning model. The first step of this process is to standardize the data and eliminate any characters that could cause problems later on in your analytic pipeline.\n",
        "\n",
        "Here, we will be working with a new dataset containing the inaugural speeches of the presidents of the United States loaded as `speech_df`, with the speeches stored in the `text` column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFmYNs7W3hYj",
        "colab_type": "code",
        "outputId": "e5915ecf-c1d6-4d24-8953-35d4afc04e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "speech_df = pd.read_csv('https://raw.githubusercontent.com/shala2020/shala2020.github.io/master/Lecture_Materials/Assignments/MachineLearning/L6/inaugural_speeches.csv')\n",
        "speech_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Inaugural Address</th>\n",
              "      <th>Date</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>George Washington</td>\n",
              "      <td>First Inaugural Address</td>\n",
              "      <td>Thursday, April 30, 1789</td>\n",
              "      <td>Fellow-Citizens of the Senate and of the House...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>George Washington</td>\n",
              "      <td>Second Inaugural Address</td>\n",
              "      <td>Monday, March 4, 1793</td>\n",
              "      <td>Fellow Citizens:  I AM again called upon by th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>John Adams</td>\n",
              "      <td>Inaugural Address</td>\n",
              "      <td>Saturday, March 4, 1797</td>\n",
              "      <td>WHEN it was first perceived, in early times, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Thomas Jefferson</td>\n",
              "      <td>First Inaugural Address</td>\n",
              "      <td>Wednesday, March 4, 1801</td>\n",
              "      <td>Friends and Fellow-Citizens:  CALLED upon to u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thomas Jefferson</td>\n",
              "      <td>Second Inaugural Address</td>\n",
              "      <td>Monday, March 4, 1805</td>\n",
              "      <td>PROCEEDING, fellow-citizens, to that qualifica...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Name  ...                                               text\n",
              "0  George Washington  ...  Fellow-Citizens of the Senate and of the House...\n",
              "1  George Washington  ...  Fellow Citizens:  I AM again called upon by th...\n",
              "2         John Adams  ...  WHEN it was first perceived, in early times, t...\n",
              "3   Thomas Jefferson  ...  Friends and Fellow-Citizens:  CALLED upon to u...\n",
              "4   Thomas Jefferson  ...  PROCEEDING, fellow-citizens, to that qualifica...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvbCog623SDx",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Print the first 5 rows of the `text` column in `speech_df` DataFrame to see the free text fields.\n",
        "* Replace all non letter characters in the `text` column with a whitespace and add it as a new column `text_clean` in the `speech_df` DataFrame. \n",
        "* Make all characters in the newly created `text_clean` column lower case.\n",
        "* Print the first 5 rows of the `text_clean` column. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag6i4ijT4tcM",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### High level text features\n",
        "---\n",
        "Once the text has been cleaned and standardized we can begin creating features from the data. The most fundamental information we can calculate about free form text is its size, such as its length and number of words. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4jPqRmA4zRA",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Record the character length of each speech (`speech_df['text_clean']`) and store it in a new `char_count` column.\n",
        "* Record the word count of each speech in the `word_count` column.\n",
        "* Record the average word length of each speech in the `avg_word_length` column.\n",
        "* Print the first 5 rows of the columns: `text_clean`, `char_cnt`, `word_cnt`, `avg_word_length`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi2hOh045bsm",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Counting words (I)\n",
        "---\n",
        "Once high level information has been recorded we can begin creating features based on the actual content of each text, as given below: \n",
        "\n",
        "* For each unique word in the dataset a column is created.\n",
        "* For each entry, the number of times this word occurs is counted and the count value is entered into the respective column.\n",
        "\n",
        "These \"count\" columns can then be used to train machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6sMPuHi5nB7",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Import `CountVectorizer` from `sklearn.feature_extraction.text`.\n",
        "* Instantiate `CountVectorizer` and assign it to 'cv'.\n",
        "* Fit the vectorizer to the `text_clean` column.\n",
        "* Print the feature names generated by the vectorizer and find the number of features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nomeBUIc5lA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Instantiate CountVectorizer\n",
        "cv = CountVectorizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAm6_JHr6Cto",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Counting words (II)\n",
        "---\n",
        "Once the vectorizer has been fit to the data, it can be used to transform the text to an array representing the word counts. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcFMh9k_6Hu4",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Apply the vectorizer ('cv' in the previous exercise) to the `text_clean` column.\n",
        "* Convert this transformed (sparse) array into a `numpy` array with counts and print it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUQAwgOV73BB",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Limiting your features\n",
        "---\n",
        "As we have seen, using the `CountVectorizer` with its default settings creates a feature for every single word in our corpus. This can create far too many features, often including ones that will provide very little analytical value.\n",
        "\n",
        "For this purpose `CountVectorizer` has parameters that you can set to reduce the number of features:\n",
        "\n",
        "* `min_df` : Use only words that occur in more than this percentage of documents. This can be used to remove outlier words that will not generalize across texts.\n",
        "* `max_df` : Use only words that occur in less than this percentage of documents. This is useful to eliminate very common words that occur in every corpus without adding value such as \"and\" or \"the\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9X2x50g8Dfd",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Limit the number of features in the `CountVectorizer` by setting the minimum number of documents a word can appear to 20% and the maximum to 80%.\n",
        "* Fit and apply the vectorizer on `text_clean` column in one step.\n",
        "* Convert this transformed (sparse) array into a `numpy` array with counts and print the dimensions of the new reduced array.\n",
        "* Did you notice that the number of features (unique words) greatly reduced from 9043 to 818?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqNCtZu-8NbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Specify arguements to limit the number of features generated\n",
        "cv = CountVectorizer(min_df=0.2, max_df=0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKZ4xX3L9IxL",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Text to DataFrame\n",
        "---\n",
        "Now that we have generated these count based features in an array we will need to reformat them so that they can be combined with the rest of the dataset. This can be achieved by converting the array into a pandas DataFrame, with the feature names you found earlier as the column names, and then concatenate it with the original DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvioGe8x9Olj",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Create a DataFrame `cv_df` containing the `cv_array` as the values and the feature names as the column names.\n",
        "* Add the prefix `Counts_` to the column names for ease of identification.\n",
        "* Concatenate this DataFrame (`cv_df`) to the original DataFrame (`speech_df`) column wise.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rpo0ek00-dsr",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Tf-idf\n",
        "---\n",
        "While counts of occurrences of words can be useful to build models, words that occur many times may skew the results undesirably. To limit these common words from overpowering your model a form of normalization can be used. In this lesson we will be using **Term frequency-inverse document frequency** (**Tf-idf**). Tf-idf has the effect of reducing the value of common words, while increasing the weight of words that do not occur in many documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c29xSq9Q-mcH",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Import `TfidfVectorizer` from `sklearn.feature_extraction.text`.\n",
        "* Instantiate `TfidfVectorizer` while limiting the number of features to 100 and removing English stop words.\n",
        "* Fit and apply the vectorizer on `text_clean` column in one step.\n",
        "* Create a DataFrame `tv_df` containing the weights of the words and the feature names as the column names.\n",
        "* Add the prefix `TFIDF_` to the column names for ease of identification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F0vdfx9-yIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Instantiate TfidfVectorizer\n",
        "tv = TfidfVectorizer(max_features=100, stop_words='english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzcGgNmS_Yc5",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Inspecting Tf-idf values\n",
        "---\n",
        "After creating Tf-idf features we will often want to understand what are the most highest scored words for each corpus. This can be achieved by isolating the row we want to examine and then sorting the the scores from high to low."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP0QjLZJ_cvX",
        "colab_type": "text"
      },
      "source": [
        "* Assign the first row of `tv_df` to `sample_row`.\n",
        "* `sample_row` is now a series of weights assigned to words. Sort these values to print the top 5 highest-rated words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g800sJg2_j2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Isolate the row to be examined\n",
        "sample_row = tv_df.iloc[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7WK0CHgH_3J",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Sentiment analysis\n",
        "---\n",
        "You have been given the tweets about US airlines. Making use of this data, your task is to predict whether a tweet contains \n",
        "\n",
        "* positive, \n",
        "* negative, or \n",
        "* neutral sentiment \n",
        "\n",
        "about the airline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mupsP8kXIFld",
        "colab_type": "code",
        "outputId": "6e45ae9b-bbca-47c0-c351-ae719400989b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "data_source_url = \"https://raw.githubusercontent.com/shala2020/shala2020.github.io/master/Lecture_Materials/Assignments/MachineLearning/L6/Tweets.csv\"\n",
        "airline_tweets = pd.read_csv(data_source_url)\n",
        "airline_tweets.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:36 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:14:45 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id  ...               user_timezone\n",
              "0  570306133677760513  ...  Eastern Time (US & Canada)\n",
              "1  570301130888122368  ...  Pacific Time (US & Canada)\n",
              "2  570301083672813571  ...  Central Time (US & Canada)\n",
              "3  570301031407624196  ...  Pacific Time (US & Canada)\n",
              "4  570300817074462722  ...  Pacific Time (US & Canada)\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYlKJmRzIHlu",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Draw a pie chart to see the number of tweets for each airline. \n",
        "* Draw a pie chart to see the distribution of sentiments across all the tweets. \n",
        "* Apply suitable data pre-processing steps to get rid of undesired symbols. This part has been done upto some extent.  \n",
        "* Using `TfidfVectorizer` class, convert text features into TF-IDF feature vectors.\n",
        "* `airline_sentiment` is the label and `text` is the feature. Apply suitable `train_test_split`, implement suitable machine learning classifier, and show the accuracy. \n",
        "\n",
        "Note: \n",
        "\n",
        "* For your convenience, the pre-processing of features have been done. \n",
        "* You are supposed to apply the vectorization on `processed_features` and implement ML algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5W-u3BCBfRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracting features and label\n",
        "features = airline_tweets.iloc[:, 10].values\n",
        "labels = airline_tweets.iloc[:, 1].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeDEmJ8yBiQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re \n",
        "\n",
        "processed_features = []\n",
        "\n",
        "for sentence in range(0, len(features)):\n",
        "    # Remove all the special characters\n",
        "    processed_feature = re.sub(r'\\W', ' ', str(features[sentence]))\n",
        "\n",
        "    # remove all single characters\n",
        "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "\n",
        "    # Remove single characters from the start\n",
        "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
        "\n",
        "    # Substituting multiple spaces with single space\n",
        "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
        "\n",
        "    # Removing prefixed 'b'\n",
        "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
        "\n",
        "    # Converting to Lowercase\n",
        "    processed_feature = processed_feature.lower()\n",
        "\n",
        "    processed_features.append(processed_feature)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}