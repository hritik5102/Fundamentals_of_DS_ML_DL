{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG16_Architecture.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr0eQbJeGxaH",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### **VGG16 – Convolutional Network for Classification and Detection**\n",
        "---\n",
        "\n",
        "<p align=\"center\"><img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\" width=\"60%\"/>\n",
        "\n",
        "VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper “Very Deep Convolutional Networks for Large-Scale Image Recognition”. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes. It was one of the famous model submitted to ILSVRC-2014. It makes the improvement over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3×3 kernel-sized filters one after another. VGG16 was trained for weeks and was using NVIDIA Titan Black GPU’s."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ny9N_dXGxWL",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16.png\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8q_8g7yGxUf",
        "colab_type": "text"
      },
      "source": [
        "ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazon’s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images. ImageNet consists of variable-resolution images. Therefore, the images have been down-sampled to a fixed resolution of 256×256. Given a rectangular image, the image is rescaled and cropped out the central 256×256 patch from the resulting image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIhWAZj8GxQ6",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### **Architecture**\n",
        "---\n",
        "\n",
        "The VGG16 architecture consists of twelve convolutional layers, some of which are followed by maximum pooling layers and then four fully-connected layers and finally a 1000-way softmax classifier.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://www.pyimagesearch.com/wp-content/uploads/2017/03/imagenet_vggnet_table1.png\" width=\"60%\"/>\n",
        "\n",
        "**First and Second Layers** :\n",
        "\n",
        "The input for AlexNet is a 224x224x3 RGB image which passes through first and second convolutional layers with 64 feature maps or filters having size 3×3 and same pooling with a stride of 14. The image dimensions changes to 224x224x64.\n",
        "Then the VGG16 applies maximum pooling layer or sub-sampling layer with a filter size 3×3 and a stride of two. The resulting image dimensions will be reduced to 112x112x64.\n",
        "\n",
        "**Third and Fourth Layer** :\n",
        "\n",
        "Next, there are two convolutional layer with 128 feature maps having size 3×3 and a stride of 1.\n",
        "Then there is again a maximum pooling layer with filter size 3×3 and a stride of 2. This layer is same as previous pooling layer except it has 128 feature maps so the output will be reduced to 56x56x128.\n",
        "\n",
        "**Fifth and Sixth Layers** :\n",
        "\n",
        "The fifth and sixth layers are convolutional layers with filter size 3×3 and a stride of one. Both used 256 feature maps.\n",
        "The two convolutional layers are followed by a maximum pooling layer with filter size 3×3, a stride of 2 and have 256 feature maps.\n",
        "\n",
        "**Seventh to Twelveth Layer** :\n",
        "\n",
        "Next are the two sets of 3 convolutional layers followed by a maximum pooling layer. All convolutional layers have 512 filters of size 3×3 and a stride of one. The final size will be reduced to 7x7x512.\n",
        "\n",
        "**Thirteenth Layer** :\n",
        "\n",
        "The convolutional layer output is flatten through a fully connected layer with 25088 feature maps each of size 1×1.\n",
        "\n",
        "**Fourteenth and Fifteenth Layers** :\n",
        "\n",
        "Next is again two fully connected layers with 4096 units.\n",
        "\n",
        "**Output Layer** :\n",
        "\n",
        "Finally, there is a softmax output layer ŷ with 1000 possible values.\n",
        "1x1x1000 thus represent contains 1000 channels (one for each class).\n",
        "\n",
        "**Activation** :\n",
        "\n",
        "All hidden layers are equipped with the rectification (ReLU) non-linearity. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4yHlFIKRsBS",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### **Summary of VGG16 Architecture**\n",
        "---\n",
        "<br/>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://engmrk.com/wp-content/uploads/2018/10/VGG16_Summary-Table.jpg\" width=\"60%\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hepxnBy0GxK2",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### **Generate a vgg16 model using keras** \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeIqrZCXGPtC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e75e1be3-6080-4f78-f653-75307c9b8cc5"
      },
      "source": [
        "## Import package\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUQe8v6vSyn7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "597b3e4d-3737-489e-eb81-cbb08483e7eb"
      },
      "source": [
        "print(\"[INFO] Model architecture ... \")\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1st layer\n",
        "model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "# 2nd layer\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "# 3rd layer\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "# 4th layer\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "# 5th layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "# 6th layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "# 7th layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "# 8th layer\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "# 9th layer\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "# 10th layer\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "# 11th layer\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "# 12th layer\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "# 13th layer\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "# 14th layer\n",
        "model.add(Dense(units=4096,activation=\"relu\"))\n",
        "\n",
        "# 15th layer\n",
        "model.add(Dense(units=4096,activation=\"relu\"))\n",
        "\n",
        "# 16th layer\n",
        "model.add(Dense(units=2, activation=\"softmax\"))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Model architecture ... \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9zc0h1mUmpw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "outputId": "8eeb1c3d-58ab-4690-d186-4af4c0c81342"
      },
      "source": [
        "print(\"[INFO] Model Summary ... \")\n",
        "model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Model Summary ... \n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 8194      \n",
            "=================================================================\n",
            "Total params: 134,268,738\n",
            "Trainable params: 134,268,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGNcBot-GxPD",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### **Drawback of vgg16 architecture**:\n",
        "---\n",
        "\n",
        "Unfortunately, there are two major drawbacks with VGGNet\n",
        "\n",
        "* It is painfully slow to train.\n",
        "\n",
        "* The network architecture weights themselves are quite large (in terms of disk/bandwidth).\n",
        "\n",
        "Due to its depth and number of fully-connected nodes, VGG is over 533MB for VGG16 and 574MB for VGG19. This makes deploying VGG a tiresome task.\n",
        "\n",
        "We still use VGG in many deep learning image classification problems; however, smaller network architectures are often more desirable (such as SqueezeNet, GoogLeNet, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXEPPECKUzlF",
        "colab_type": "text"
      },
      "source": [
        "**References**\n",
        "\n",
        "* [VGG16 – Convolutional Network for Classification and Detection](https://neurohive.io/en/popular-networks/vgg16/)\n",
        "\n",
        "* [VGG16 – Implementation Using Keras](https://engmrk.com/vgg16-implementation-using-keras/)\n",
        "\n",
        "* [PyimageSearch - imagenet-vggnet-resnet-inception-xception-keras](https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/)\n",
        "\n",
        "* [Step by step VGG16 implementation in Keras for beginners](https://towardsdatascience.com/step-by-step-vgg16-implementation-in-keras-for-beginners-a833c686ae6c)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmcJ4C_fUvbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}