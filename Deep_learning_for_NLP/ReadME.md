## NLP Resources Manual &nbsp; [![Awesome](https://awesome.re/badge.svg)](https://github.com/hritik5102/Fundamentals_of_DS_ML_DL)

This repository contains landmark research papers and blogs in Natural Language Processing that came out in this century.

## Contents

* [List of Research Papers](#list-of-research-papers)

* [Blog Posts](#blog-posts)

*  [Papers](#papers)

*  [Machine Translation](#machine-translation)

* [Language Models](#language-models)

* [Word Embeddings](#word-embeddings)

* [Image to Text](#image-to-text)

* [Transformers](#transformers)

* [List of blogs](#list-of-blogs)

* [Machine Translation](#machine-translation-1)

* [Image to Text](#image-to-text-1)

* [Transformers](#transformers-1)

* [Datasets](#datasets)

* [References](#references)
  
* [Back to Top](#back-to-top)

## List of Research Papers


## Blog Posts

1. [Natural Language Processing (NLP) progress](https://nlpprogress.com/) Tracking the most common NLP tasks, including the datasets and the current state-of-the-art 
2. [A Review of the Recent History of Natural Language Processing](http://blog.aylien.com/a-review-of-the-recent-history-of-natural-language-processing/)
3. [Deep Learning, NLP, and Representations](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)
4. [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
5. [Neural Language Modeling From Scratch](http://ofir.io/Neural-Language-Modeling-From-Scratch/?a=1)
6. [Machine Learning for Emoji Trends](http://instagram-engineering.tumblr.com/post/117889701472/emojineering-part-1-machine-learning-for-emoji)
7. [Teaching Robots to Feel: Emoji & Deep Learning](http://getdango.com/emoji-and-deep-learning.html)
8. [Computational Linguistics and Deep Learning](http://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00239) - Opinion piece on how Deep Learning fits into the broader picture of text processing.
9. [Deep Learning NLP Best Practices](http://ruder.io/deep-learning-nlp-best-practices/index.html)
10. [7 types of Artificial Neural Networks for Natural Language Processing](https://medium.com/@datamonsters/artificial-neural-networks-for-natural-language-processing-part-1-64ca9ebfa3b2)
11. [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)

## Papers

* [Deep or shallow, NLP is breaking out](http://dl.acm.org/citation.cfm?id=2874915) - General overview of how Deep Learning is impacting NLP.
* [Natural Language Processing from Research at Google](http://research.google.com/pubs/NaturalLanguageProcessing.html) - Not all Deep Learning (but mostly).
* [Context Dependent Recurrent Neural Network Language Model](http://www.msr-waypoint.com/pubs/176926/rnn_ctxt.pdf)
* [Translation Modeling with Bidirectional Recurrent Neural Networks](https://www-i6.informatik.rwth-aachen.de/publications/download/936/SundermeyerMartinAlkhouliTamerWuebkerJoernNeyHermann--TranslationModelingwithBidirectionalRecurrentNeuralNetworks--2014.pdf)
* [Contextual LSTM (CLSTM) models for Large scale NLP tasks](https://arxiv.org/abs/1602.06291)
* [LSTM Neural Networks for Language Modeling](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.248.4448&rep=rep1&type=pdf)
* [Exploring the Limits of Language Modeling](http://arxiv.org/pdf/1602.02410.pdf)
* [Conversational Contextual Cues](https://arxiv.org/abs/1606.00372) - Models context and participants in conversations.
* [Sequence to sequence learning with neural networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)
* [Efficient Estimation of Word Representations in Vector Space](http://arxiv.org/pdf/1301.3781.pdf)
* [Learning Character-level Representations for Part-of-Speech Tagging](http://jmlr.org/proceedings/papers/v32/santos14.pdf)
* [Representation Learning for Text-level Discourse Parsing](http://www.cc.gatech.edu/~jeisenst/papers/ji-acl-2014.pdf)
* [Fast and Robust Neural Network Joint Models for Statistical Machine Translation](http://acl2014.org/acl2014/P14-1/pdf/P14-1129.pdf)
* [Parsing With Compositional Vector Grammars](http://www.socher.org/index.php/Main/ParsingWithCompositionalVectorGrammars)
* [Smart Reply: Automated Response Suggestion for Email](https://arxiv.org/abs/1606.04870)
* [Neural Architectures for Named Entity Recognition](https://arxiv.org/abs/1603.01360) - State-of-the-art performance in NER with bidirectional LSTM with a sequential conditional random layer and transition-based parsing with stack LSTMs.
* [Grammar as a Foreign Language](https://arxiv.org/abs/1412.7449) - State-of-the-art syntactic constituency parsing using generic sequence-to-sequence approach.



## Machine Translation 

* [Sequence to Sequence Learning with Neural Network](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) - LSTMN based approach for sequence problems.
* [Learning Phase Representations using RNN Encoder-Decoder for statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf)
* [Attention Model(Neural Machine Translation By Jointly learning to Align and Translate)](https://arxiv.org/pdf/1409.0473.pdf) - Attention model architecture modified version for encoder decoder models (Don't confuse with [<i><b>Attention is all you need</b> paper</i>](#Transformers) i.e, for transformers concept)
* [Understanding Back-Translation at Scale](https://arxiv.org/pdf/1808.09381.pdf)
* [MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning](https://arxiv.org/abs/1911.09483)
* [Scaling Neural Machine Translation](https://arxiv.org/abs/1806.00187)
* [The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation](https://arxiv.org/abs/1804.09849)
* [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122)- Modified Attention model with convolutional layer


## Language Models

* [Scalable Hierarchical Distributed Language Model](https://www.cs.toronto.edu/~amnih/papers/hlbl_final.pdf)
* [Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf) - fastText(by Facebook AI Research) trained on billion of words for text classification. 
* [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
* [Hierarchical Probabilistic Neural Network Language Model](https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf) - Speed up training and recogintion(by 200) - Yoshua Bengio  


## Word Embeddings

* [Distributed Representations of Sentences and Documents](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) - Sentence/Document to vectors by Tomas Mikolov by Google
* [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)- WOrd2Vec representation by Tomas Mikolov(Google)
* [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf) - High quality vector representation from huge data sets by Tomas Mikolov(Google)
* [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)- based on deep birectional Language Model by Allen Institute for Artificial Intelligence
* [Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf) - Handles morphology and generates vectors for words not present in training dataset by Facebook AI Research.
* [Misspelling Oblivious Word Embeddings](https://arxiv.org/abs/1905.09755)

## Image to Text

* [Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf)
* [Deep Visual-Semantic Alignments for Generating Image Descriptions](https://cs.stanford.edu/people/karpathy/cvpr2015.pdf)


## Transformers

* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) - Multimodal Recurrent Neural Network architecture for image description by [Andrej Kaparthy](http://karpathy.github.io/) and Le-Fei-fei


## List of blogs

## Machine Translation

* [Google Machine Translation Blog](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html)
* [Email AutoReply and Auto Suggestion](https://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html)
* [Find Code errors and repair](https://medium.com/@martin.monperrus/sequence-to-sequence-learning-program-repair-e39dc5c0119b)



## Image to Text
* [Image Captioning Using Keras](https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8)


## Transformers
* [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Transformers Research paper core details explained by [Jalammar](http://jalammar.github.io/)
* [The Illustrated BERT](http://jalammar.github.io/illustrated-bert/) - BERT is explained by [Jalammar](http://jalammar.github.io/)
* [A Visual Guide to Using BERT for the First Time :boom:](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/) - Very beautifully explained BERT architecture with the help of visuals. 

## Datasets

1. [Dataset from "One Billion Word Language Modeling Benchmark"](http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz) - Almost 1B words, already pre-processed text.
2. [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/treebank.html) - Fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences.
3. [Chatbot data from Kaggle](https://www.kaggle.com/samdeeplearning/deepnlp)
4. [A list of text datasets that are free/public domain in alphabetical order](https://github.com/niderhoff/nlp-datasets)
5. [Another list of text datasets that are free/public domain in reverse chronological order](https://github.com/karthikncode/nlp-datasets)
6. Question Answering datasets
	1. [Quora's Question Pairs Dataset](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) - Identify question pairs that have the same intent.
	2. [CMU's Wikipedia Factoid Question Answers](https://www.cs.cmu.edu/~ark/QA-data/)
	3. [DeepMind's Algebra Question Answering](https://github.com/deepmind/AQuA)
	4. [DeepMind's from CNN & DailyMail Question Answering](https://github.com/deepmind/rc-data)
	5. [Microsoft's WikiQA Open Domain Question Answering](https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/)
	6. [Stanford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/) - covering reading comprehension

7. https://bigquery.cloud.google.com/table/fh-bigquery:reddit_comments.2015_08?pli=1 : Reddit comments dataset

8. https://code.google.com/archive/p/word2vec/ : Links to unlabelled english corpus

9. http://github.com/brmson/dataset-sts : Variety of datasets wrapped in Python with focus on comparing two sentences, sample implementations of popular deep NN models in Keras

10. http://www.mpi-sws.org/~cristian/Cornell_Movie-Dialogs_Corpus.html : Conversation dataset (for learning seq2seq models possible leading to a chatbot kind of application)

11. https://github.com/rkadlec/ubuntu-ranking-dataset-creator : Ubuntu Dialog Corpus 
 5.1 : http://arxiv.org/pdf/1506.08909v3.pdf : Accompanying paper for Ubuntu dataset

12. http://www.aclweb.org/anthology/P12-2040 : Another Dialogue corpus

13. http://www.lrec-conf.org/proceedings/lrec2012/pdf/1114_Paper.pdf : yet another dialogue corpus
8. http://www.cs.technion.ac.il/~gabr/resources/data/ne_datasets.html : NER resources

14. http://linguistics.cornell.edu/language-corpora : List of NLP resources

15. https://github.com/aritter/twitter_nlp/blob/master/data/annotated/ner.txt : Annotated twitter corpus

16. http://schwa.org/projects/resources/wiki/Wikiner

17. https://www.aclweb.org/anthology/W/W10/W10-0712.pdf : Paper describing annotation process for NER on large email data (could not find any link, if anyone finds out please feel free to send a PR)

18. http://www.cs.cmu.edu/~mgormley/papers/napoles+gormley+van-durme.naaclw.2012.pdf : Annotated gigawords

19. http://jmcauley.ucsd.edu/data/amazon/ : Amazon review dataset (LARGE CORPUS)

20. http://curtis.ml.cmu.edu/w/courses/index.php/Amazon_product_reviews_dataset : Amazon product review dataset (available only on request)

21. http://times.cs.uiuc.edu/~wang296/Data/ : Amazon review dataset

22. https://www.yelp.com/dataset_challenge : Yelp dataset (review + images)



## References

Major resources are reference from this repository which is open source in github

**Check this out**  ðŸ‘‡

* [Robofied - Awesome NLP Resource](https://github.com/Robofied/Awesome-NLP-Resources)
* [shashankg7 - Deep Learning for NLP Resources](https://github.com/shashankg7/Deep-Learning-for-NLP-Resources)
* [brianspiering - awesome-dl4nlp](https://github.com/brianspiering/awesome-dl4nlp)

### [Back to Top](#Contents)